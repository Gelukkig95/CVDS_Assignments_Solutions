{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "315226be-94bc-4964-ba77-39d775ede1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d4638fe-6814-4954-beed-ebf3d16d2fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(100, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 784),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        output = output.view(x.size(0), 1, 28, 28)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ed7e3fc-9239-4cc1-bae8-5f1a8520873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "  \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(784, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), 784)\n",
    "        output = self.model(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae97fee3-59ec-4d56-bf30-df052fc92c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss D: 0.0485 | Loss G: 4.0814\n"
     ]
    }
   ],
   "source": [
    "def train_gan(\n",
    "    batch_size: int = 32,\n",
    "    num_epochs: int = 100,\n",
    "    device: str = \"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
    "):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    train_set = torchvision.datasets.MNIST(\n",
    "        root=\".\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    discriminator = Discriminator().to(device)\n",
    "    generator = Generator().to(device)\n",
    "\n",
    "    loss_function = nn.BCELoss()\n",
    "    optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=0.0001)\n",
    "    optimizer_g = torch.optim.Adam(generator.parameters(), lr=0.0001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for n, (real_samples, _) in enumerate(train_loader):\n",
    "\n",
    "    \n",
    "            current_batch_size = real_samples.size(0)\n",
    "\n",
    "            real_samples = real_samples.to(device)\n",
    "\n",
    "            real_labels = torch.ones((current_batch_size, 1), device=device)\n",
    "            fake_labels = torch.zeros((current_batch_size, 1), device=device)\n",
    "\n",
    "            latent_vectors = torch.randn((current_batch_size, 100), device=device)\n",
    "            fake_samples = generator(latent_vectors)\n",
    "\n",
    "         \n",
    "            all_samples = torch.cat((real_samples, fake_samples))\n",
    "            all_labels = torch.cat((real_labels, fake_labels))\n",
    "\n",
    "            discriminator.zero_grad()\n",
    "            output = discriminator(all_samples)\n",
    "            loss_d = loss_function(output, all_labels)\n",
    "            loss_d.backward()\n",
    "            optimizer_d.step()\n",
    "\n",
    "           \n",
    "            latent_vectors = torch.randn((current_batch_size, 100), device=device)\n",
    "            generator.zero_grad()\n",
    "            generated = generator(latent_vectors)\n",
    "            output = discriminator(generated)\n",
    "            loss_g = loss_function(output, real_labels)\n",
    "            loss_g.backward()\n",
    "            optimizer_g.step()\n",
    "\n",
    "       \n",
    "        clear_output(wait=True)\n",
    "        print(f\"Epoch {epoch} | Loss D: {loss_d:.4f} | Loss G: {loss_g:.4f}\")\n",
    "\n",
    "        samples = generated[:16].detach().cpu()\n",
    "        fig = plt.figure(figsize=(4, 4))\n",
    "        for i in range(16):\n",
    "            ax = fig.add_subplot(4, 4, i + 1)\n",
    "            ax.imshow(samples[i].reshape(28, 28), cmap=\"gray_r\")\n",
    "            ax.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "train_gan(batch_size=32, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5fd345-697e-4573-b7b0-1dc7d19db681",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_env",
   "language": "python",
   "name": "cv_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
